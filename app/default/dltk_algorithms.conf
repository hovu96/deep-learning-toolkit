[default]
runtime = 
description = 
category = 
source_code = 
source_code_version = 0
deployment_code = 
deployment_code_version = 0
# https://docs.splunk.com/Documentation/Splunk/latest/Search/Typesofcommands
# https://conf.splunk.com/files/2017/slides/extending-spl-with-custom-search-commands-and-the-splunk-sdk-for-python.pdf
# streaming: one-by-one, can be pushed to indexers
# stateful: one-by-one, sh-only, no re-ordering
# events: sh-only, may re-order
# reporting: sh-only, for stats/etc
command_type = reporting
default_method = 
max_buffer_size = auto
support_preop = false

# mltk fit: EVENTS
# mltk apply: streaming or stateful

[Binary Neural Network Classifier:summary]
[Binary Neural Network Classifier:apply]
[Binary Neural Network Classifier:fit]
[Binary Neural Network Classifier]
runtime = base
description = Binary neural network classifier build on keras and TensorFlow
category = Classifier
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Notebook for TensorFlow 2.0"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Neural Network for Binary Classification\n",\
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk. As an example we use a custom binary neural network classifier built on keras and tensorflow."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 1,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_import\n",\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import tensorflow as tf\n",\
    "from tensorflow import keras\n",\
    "\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 2,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "numpy version: 1.16.2\n",\
      "pandas version: 0.24.2\n",\
      "TensorFlow version: 2.0.0-alpha0\n",\
      "Keras version: 2.2.4-tf\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"TensorFlow version: \" + tf.__version__)\n",\
    "print(\"Keras version: \" + keras.__version__)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a prepared dataset into this environment."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup diabetes.csv <br>| fit MLTKContainer response from * algo=binary_nn_classifier epochs=10 mode=stage into MyModel"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"my_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_stage\n",\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "df, param = stage(\"default\")\n",\
    "print(df[0:1])\n",\
    "print(df.shape)\n",\
    "print(str(param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_init\n",\
    "# initialize the model\n",\
    "# params: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    print(\"FIT build model with input shape \" + str(X.shape))\n",\
    "    input_shape = int(X.shape[1])\n",\
    "    model_structure = '2-2'\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'structure' in param['options']['params']:\n",\
    "                model_structure = str(param['options']['params']['structure']).lstrip(\"\\\"\").rstrip(\"\\\"\")\n",\
    "    hidden_factors = np.floor(np.array(model_structure.split(\"-\"), dtype=\"float\") * X.shape[1])\n",\
    "    model = keras.Sequential()\n",\
    "    model.add(keras.layers.Dense(input_shape, input_dim=input_shape, activation=tf.nn.relu))\n",\
    "    for hidden in hidden_factors:\n",\
    "        model.add(keras.layers.Dense(int(hidden), activation=tf.nn.relu))\n",\
    "    model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",\
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "# test mltkc_stage_create_model\n",\
    "model = init(df,param)\n",\
    "print(model.summary())"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_stage_create_model_fit\n",\
    "# returns a fit info json object\n",\
    "def fit(model,df,param):\n",\
    "    returns = {}\n",\
    "    X = df[param['feature_variables']]\n",\
    "    Y = df[param['target_variables']]\n",\
    "    model_epochs = 100\n",\
    "    model_batch_size = None\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model_epochs = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model_batch_size = int(param['options']['params']['batch_size'])\n",\
    "    # connect model training to tensorboard\n",\
    "    log_dir=\"/srv/notebooks/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",\
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",\
    "    # run the training\n",\
    "    returns['fit_history'] = model.fit(x=X,\n",\
    "                                       y=Y, \n",\
    "                                       verbose=2, \n",\
    "                                       epochs=model_epochs, \n",\
    "                                       batch_size=model_batch_size, \n",\
    "                                       #validation_data=(X, Y),\n",\
    "                                       callbacks=[tensorboard_callback])\n",\
    "    # memorize parameters\n",\
    "    returns['model_epochs'] = model_epochs\n",\
    "    returns['model_batch_size'] = model_batch_size\n",\
    "    returns['model_loss_acc'] = model.evaluate(x = X, y = Y)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = fit(model,df,param)\n",\
    "print(returns['model_loss_acc'])"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_stage_create_model_apply\n",\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    y_hat = model.predict(x = X, verbose=1)\n",\
    "    return pd.concat([df,pd.DataFrame(y_hat)], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# test mltkc_stage_create_model_apply\n",\
    "y_hat = apply(model,df,param)\n",\
    "print(y_hat)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def save(model,name):\n",\
    "    # save keras model to hdf5 file\n",\
    "    # https://www.tensorflow.org/beta/tutorials/keras/save_and_restore_models\n",\
    "    model.save(MODEL_DIRECTORY + name + \".h5\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def load(name):\n",\
    "    model = keras.models.load_model(MODEL_DIRECTORY + name + \".h5\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"tensorflow\": tf.__version__, \"keras\": keras.__version__} }\n",\
    "    if model is not None:\n",\
    "        # Save keras model summary to string:\n",\
    "        s = []\n",\
    "        model.summary(print_fn=lambda x: s.append(x+'\\n'))\n",\
    "        returns[\"summary\"] = ''.join(s)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 2

[Logistic Regression:summary]
[Logistic Regression:apply]
[Logistic Regression:fit]
[Logistic Regression]
runtime = base
description = Logistic Regression in PyTorch
category = Classifier
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Notebook for PyTorch"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Logistic Regression in PyTorch\n",\
    "This notebook contains an example for a simple logistic regression in PyTorch.<br>By default every time you save this notebook the cells are exported into a python module which is then used for executing your custom model invoked by Splunk MLTK Container App. "\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import scipy as sp\n",\
    "import pandas as pd\n",\
    "import torch\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"scipy version: \" + sp.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"PyTorch: \" + torch.__version__)\n",\
    "if torch.cuda.is_available():\n",\
    "    print(f\"There are {torch.cuda.device_count()} CUDA devices available\")\n",\
    "    for i in range(0,torch.cuda.device_count()):\n",\
    "        print(f\"Device {i:0}: {torch.cuda.get_device_name(i)} \")\n",\
    "else:\n",\
    "    print(\"No GPU found\")"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a prepared sample dataset into this environment."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup iris.csv <br>\n",\
    "| fit MLTKContainer algo=pytorch_logistic_regression epochs=1000 mode=stage species from petal_length petal_width sepal_length sepal_width into app:PyTorch_iris_model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"PyTorch_iris_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "df, param = stage(\"PyTorch_iris_model\")\n",\
    "#print(param)\n",\
    "print(df.describe)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "def init(df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    Y = df[param['target_variables']]\n",\
    "    input_size = int(X.shape[1])\n",\
    "    num_classes = len(np.unique(Y.to_numpy()))\n",\
    "    learning_rate = 0.001\n",\
    "    mapping = { key: value for value,key in enumerate(np.unique(Y.to_numpy().reshape(-1))) }\n",\
    "    print(\"FIT build logistic regression model with input shape \" + str(X.shape))\n",\
    "    print(\"FIT build model with target classes \" + str(num_classes))\n",\
    "    model = {\n",\
    "        \"input_size\": input_size,\n",\
    "        \"num_classes\": num_classes,\n",\
    "        \"learning_rate\": learning_rate,\n",\
    "        \"mapping\": mapping,\n",\
    "        \"num_epochs\": 10000,\n",\
    "        \"batch_size\": 100,\n",\
    "    }\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model['num_epochs'] = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model['batch_size'] = int(param['options']['params']['batch_size'])\n",\
    "    # Simple logistic regression model\n",\
    "    model['model'] = torch.nn.Linear(input_size, num_classes)\n",\
    "    # Define loss and optimizer\n",\
    "    model['criterion'] = torch.nn.CrossEntropyLoss()  \n",\
    "    model['optimizer'] = torch.optim.SGD(model['model'].parameters(), lr=learning_rate)      \n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "def fit(model,df,param):\n",\
    "    returns = {}\n",\
    "    X = df[param['feature_variables']].astype('float32').to_numpy()\n",\
    "    Y = df[param['target_variables']].to_numpy().reshape(-1)\n",\
    "    mapping = { key: value for value,key in enumerate(np.unique(Y)) }\n",\
    "    Y = df[param['target_variables']].replace( {param['target_variables'][0]:mapping } ).to_numpy().reshape(-1)\n",\
    "    #Y = df[param['target_variables']].to_numpy().reshape(-1)\n",\
    "    #Y = pd.get_dummies(Y).astype('float32').to_numpy()\n",\
    "    #Ymap = df[param['target_variables']].replace( {param['target_variables'][0]:mapping } ).to_numpy()\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model['num_epochs'] = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model['batch_size'] = int(param['options']['params']['batch_size'])\n",\
    "    print(model['num_epochs'])\n",\
    "    for epoch in range(model['num_epochs']):\n",\
    "        inputs = torch.from_numpy(X)\n",\
    "        targets = torch.from_numpy(Y)\n",\
    "        outputs = model['model'](inputs)\n",\
    "        loss = model['criterion'](outputs, targets)\n",\
    "        model['optimizer'].zero_grad()\n",\
    "        loss.backward()\n",\
    "        model['optimizer'].step()\n",\
    "        if (epoch+1) % (model['num_epochs']/10) == 0:\n",\
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, model['num_epochs'], loss.item()))                \n",\
    "    # memorize parameters\n",\
    "    returns['model_epochs'] = model['num_epochs']\n",\
    "    returns['model_batch_size'] = model['batch_size']\n",\
    "    returns['model_loss_acc'] = loss.item()\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = fit(model,df,param)\n",\
    "print(returns['model_loss_acc'])"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']].astype('float32').to_numpy()\n",\
    "    classes = {v: k for k, v in model['mapping'].items()}\n",\
    "    with torch.no_grad():\n",\
    "        input = torch.from_numpy(X)\n",\
    "        output = model['model'](input)\n",\
    "        y_hat = output.data\n",\
    "        _, predicted = torch.max(output.data, 1)\n",\
    "        prediction = [classes[key] for key in predicted.numpy()]\n",\
    "    return pd.concat([df, pd.DataFrame(prediction)], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "y_hat = apply(model,df,param)\n",\
    "y_hat"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def save(model,name):\n",\
    "    torch.save(model, MODEL_DIRECTORY + name + \".pt\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def load(name):\n",\
    "    model = torch.load(MODEL_DIRECTORY + name + \".pt\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"pytorch\": torch.__version__} }\n",\
    "    if model is not None:\n",\
    "        if 'model' in model:\n",\
    "            returns[\"summary\"] = str(model)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 6


[Multi Class Neural Network Classifier:summary]
[Multi Class Neural Network Classifier:apply]
[Multi Class Neural Network Classifier:fit]
[Multi Class Neural Network Classifier]
runtime = base
description = Simple multi class neural network classifier using PyTorch with GPU
category = Classifier
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Notebook for PyTorch"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Multi class neural network in PyTorch\n",\
    "This notebook contains an example for a simple multi class neural network in PyTorch.<br>By default every time you save this notebook the cells are exported into a python module which is then used for executing your custom model invoked by Splunk MLTK Container App. "\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import scipy as sp\n",\
    "import pandas as pd\n",\
    "import torch\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"scipy version: \" + sp.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"PyTorch: \" + torch.__version__)\n",\
    "if torch.cuda.is_available():\n",\
    "    print(f\"There are {torch.cuda.device_count()} CUDA devices available\")\n",\
    "    for i in range(0,torch.cuda.device_count()):\n",\
    "        print(f\"Device {i:0}: {torch.cuda.get_device_name(i)} \")\n",\
    "else:\n",\
    "    print(\"No GPU found\")"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a prepared sample dataset into this environment."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup iris.csv <br>\n",\
    "| fit MLTKContainer mode=stage algo=pytorch_nn epochs=10 species from petal_length petal_width sepal_length sepal_width into app:PyTorch_iris_model_nn"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"PyTorch_iris_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "df, param = stage(\"PyTorch_iris_model_nn\")\n",\
    "print(df.describe)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "def init(df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    Y = df[param['target_variables']]\n",\
    "    input_size = int(X.shape[1])\n",\
    "    num_classes = len(np.unique(Y.to_numpy()))\n",\
    "    learning_rate = 0.001\n",\
    "    mapping = { key: value for value,key in enumerate(np.unique(Y.to_numpy().reshape(-1))) }\n",\
    "    print(\"FIT build neural network model with input shape \" + str(X.shape))\n",\
    "    print(\"FIT build model with target classes \" + str(num_classes))\n",\
    "    model = {\n",\
    "        \"input_size\": input_size,\n",\
    "        \"num_classes\": num_classes,\n",\
    "        \"learning_rate\": learning_rate,\n",\
    "        \"mapping\": mapping,\n",\
    "        \"num_epochs\": 10000,\n",\
    "        \"batch_size\": 100,\n",\
    "        \"hidden_layers\" : 10,\n",\
    "    }\n",\
    "    device = None\n",\
    "    if torch.cuda.is_available():\n",\
    "        device = torch.device('cuda')\n",\
    "    else:\n",\
    "        device = torch.device('cpu')\n",\
    "    model['device'] = device\n",\
    "    \n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model['num_epochs'] = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model['batch_size'] = int(param['options']['params']['batch_size'])\n",\
    "            if 'hidden_layers' in param['options']['params']:\n",\
    "                model['hidden_layers'] = int(param['options']['params']['hidden_layers'])\n",\
    "\n",\
    "    # Simple neural network model\n",\
    "    model['model'] = torch.nn.Sequential(\n",\
    "        torch.nn.Linear(model['input_size'], model['hidden_layers']),\n",\
    "        torch.nn.ReLU(),\n",\
    "        torch.nn.Linear(model['hidden_layers'], model['num_classes']),\n",\
    "    ).to(model['device'])\n",\
    "\n",\
    "    # Define loss and optimizer\n",\
    "    model['criterion'] = torch.nn.CrossEntropyLoss()  \n",\
    "    model['optimizer'] = torch.optim.SGD(model['model'].parameters(), lr=learning_rate)      \n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "def fit(model,df,param):\n",\
    "    returns = {}\n",\
    "    X = df[param['feature_variables']].astype('float32').to_numpy()\n",\
    "    Y = df[param['target_variables']].to_numpy().reshape(-1)\n",\
    "    mapping = { key: value for value,key in enumerate(np.unique(Y)) }\n",\
    "    Y = df[param['target_variables']].replace( {param['target_variables'][0]:mapping } ).to_numpy().reshape(-1)\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model['num_epochs'] = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model['batch_size'] = int(param['options']['params']['batch_size'])\n",\
    "    print(model['num_epochs'])\n",\
    "\n",\
    "    inputs = torch.from_numpy(X).to(model['device'])\n",\
    "    targets = torch.from_numpy(Y).to(model['device'])\n",\
    "\n",\
    "    for epoch in range(model['num_epochs']):\n",\
    "        outputs = model['model'](inputs)\n",\
    "        loss = model['criterion'](outputs, targets)\n",\
    "        model['optimizer'].zero_grad()\n",\
    "        loss.backward()\n",\
    "        model['optimizer'].step()\n",\
    "        if (epoch+1) % (model['num_epochs']/100) == 0:\n",\
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, model['num_epochs'], loss.item()))                \n",\
    "    # memorize parameters\n",\
    "    returns['model_epochs'] = model['num_epochs']\n",\
    "    returns['model_batch_size'] = model['batch_size']\n",\
    "    returns['model_loss_acc'] = loss.item()\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = fit(model,df,param)\n",\
    "print(returns['model_loss_acc'])"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']].astype('float32').to_numpy()\n",\
    "    classes = {v: k for k, v in model['mapping'].items()}\n",\
    "    with torch.no_grad():\n",\
    "        input = torch.from_numpy(X).to(model['device'])\n",\
    "        output = model['model'](input)\n",\
    "        y_hat = output.data\n",\
    "        _, predicted = torch.max(output.data, 1)\n",\
    "        predicted = predicted.cpu()\n",\
    "        prediction = [classes[key] for key in predicted.numpy()]\n",\
    "    return pd.concat([df, pd.DataFrame(prediction)], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "y_hat = apply(model,df,param)\n",\
    "y_hat"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def save(model,name):\n",\
    "    torch.save(model, MODEL_DIRECTORY + name + \".pt\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def load(name):\n",\
    "    model = torch.load(MODEL_DIRECTORY + name + \".pt\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"pytorch\": torch.__version__} }\n",\
    "    if model is not None:\n",\
    "        if 'model' in model:\n",\
    "            returns[\"summary\"] = str(model)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 20

[Autoencoder:summary]
[Autoencoder:apply]
[Autoencoder:fit]
[Autoencoder]
runtime = base
description = Basic auto encoder using TensorFlow™ 
category = Clustering
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Notebook for TensorFlow 2.0"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Autoencoder Example\n",\
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk. As an example we use a custom autoencoder built on keras and tensorflow."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_import\n",\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import tensorflow as tf\n",\
    "from tensorflow import keras\n",\
    "\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"TensorFlow version: \" + tf.__version__)\n",\
    "print(\"Keras version: \" + keras.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a prepared dataset into this environment."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup iris.csv <br>| fit MLTKContainer algo=autoencoder epochs=100 batch_size=4 components=2 petal_length petal_width sepal_length sepal_width into app:iris_autoencoder"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"my_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_stage\n",\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "df, param = stage(\"iris_autoencoder\")\n",\
    "print(df[0:1])\n",\
    "print(df.shape)\n",\
    "print(str(param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_init\n",\
    "# initialize the model\n",\
    "# params: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    print(\"FIT build model with input shape \" + str(X.shape))\n",\
    "    components = 3\n",\
    "    activation_fn = 'relu'\n",\
    "    # learning_rate = 0.001\n",\
    "    # epsilon=0.00001 # default 1e-07\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'components' in param['options']['params']:\n",\
    "                components = int(param['options']['params']['components'])\n",\
    "            if 'activation_func' in param['options']['params']:\n",\
    "                activation_fn = param['options']['params']['activation_func']\n",\
    "    input_shape = int(X.shape[1])\n",\
    "    encoder_layer = keras.layers.Dense(components, input_dim=input_shape, activation=activation_fn, kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None), bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))\n",\
    "    decoder_layer = keras.layers.Dense(input_shape, activation=activation_fn, kernel_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None), bias_initializer=keras.initializers.RandomUniform(minval=0, maxval=1, seed=None))\n",\
    "    model = keras.Sequential()\n",\
    "    model.add(encoder_layer)\n",\
    "    model.add(decoder_layer)\n",\
    "    #opt = keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",\
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "# test mltkc_stage_create_model\n",\
    "model = init(df,param)\n",\
    "print(model.summary())"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_stage_create_model_fit\n",\
    "# returns a fit info json object\n",\
    "def fit(model,df,param):\n",\
    "    returns = {}\n",\
    "    X = df[param['feature_variables']]\n",\
    "    model_epochs = 100\n",\
    "    model_batch_size = 32\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            if 'epochs' in param['options']['params']:\n",\
    "                model_epochs = int(param['options']['params']['epochs'])\n",\
    "            if 'batch_size' in param['options']['params']:\n",\
    "                model_batch_size = int(param['options']['params']['batch_size'])\n",\
    "    # connect model training to tensorboard\n",\
    "    log_dir=\"/srv/notebooks/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",\
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",\
    "    # run the training\n",\
    "    returns['fit_history'] = model.fit(x=X,\n",\
    "                                       y=X, \n",\
    "                                       verbose=2, \n",\
    "                                       epochs=model_epochs, \n",\
    "                                       batch_size=model_batch_size, \n",\
    "                                       #validation_data=(X, Y),\n",\
    "                                       callbacks=[tensorboard_callback])\n",\
    "    # memorize parameters\n",\
    "    returns['model_epochs'] = model_epochs\n",\
    "    returns['model_batch_size'] = model_batch_size\n",\
    "    returns['model_loss_acc'] = model.evaluate(x = X, y = X)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = fit(model,df,param)\n",\
    "print(returns['model_loss_acc'])"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# mltkc_stage_create_model_apply\n",\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    reconstruction = model.predict(x = X)\n",\
    "    intermediate_layer_model = keras.Model(inputs=model.input, outputs=model.layers[0].output)\n",\
    "    hidden = intermediate_layer_model.predict(x = X)\n",\
    "    y_hat = pd.concat([pd.DataFrame(reconstruction).add_prefix(\"reconstruction_\"), pd.DataFrame(hidden).add_prefix(\"hidden_\")], axis=1)\n",\
    "    return pd.concat([df,y_hat], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# test mltkc_stage_create_model_apply\n",\
    "y_hat = apply(model,df,param)\n",\
    "print(y_hat)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def save(model,name):\n",\
    "    # save keras model to hdf5 file\n",\
    "    # https://www.tensorflow.org/beta/tutorials/keras/save_and_restore_models\n",\
    "    model.save(MODEL_DIRECTORY + name + \".h5\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def load(name):\n",\
    "    model = keras.models.load_model(MODEL_DIRECTORY + name + \".h5\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"tensorflow\": tf.__version__, \"keras\": keras.__version__} }\n",\
    "    if model is not None:\n",\
    "        # Save keras model summary to string:\n",\
    "        s = []\n",\
    "        model.summary(print_fn=lambda x: s.append(x+'\\n'))\n",\
    "        returns[\"summary\"] = ''.join(s)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 2

[Distributed KMeans with Dask:fit]
[Distributed KMeans with Dask]
runtime = base
description = Distribute algorithm execution with DASK for KMeans
category = Clustering
source_code_version = 4
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Dask Distributed KMeans"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains a barebone example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import dask.dataframe as dd\n",\
    "from dask.distributed import Client\n",\
    "import dask_ml.cluster\n",\
    "\n",\
    "# ...\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "import dask_ml\n",\
    "print(\"dask_ml version: \" + dask_ml.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup iris.csv <br/>\n",\
    "| fit MLTKContainer algo=dask_kmeans k=3 petal_length petal_width sepal_length sepal_width into app:iris_dask_kmeans"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "df, param = stage(\"iris_dask_kmeans\")\n",\
    "print(df)\n",\
    "print(param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    model = {}\n",\
    "    \n",\
    "    #client = Client(\"tcp://127.0.0.1:\")\n",\
    "    client = Client(processes=False)\n",\
    "    \n",\
    "    model['dask_client'] = client\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    ddf = dd.from_pandas(df, npartitions=4)  \n",\
    "    # features dataframe\n",\
    "    features = ddf[param['feature_variables']]\n",\
    "    #features.persist()\n",\
    "    k = int(param['options']['params']['k'])\n",\
    "    model['dask_kmeans'] = dask_ml.cluster.KMeans(n_clusters=k, init_max_iter=2, oversampling_factor=10)\n",\
    "    model['dask_kmeans'].fit(features)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "%time print(fit(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def apply(model,df,param):\n",\
    "    #ddf = dd.from_pandas(df[param['feature_variables']], npartitions=4)\n",\
    "    prediction = model['dask_kmeans'].labels_\n",\
    "    #y_hat = prediction.to_dask_dataframe()\n",\
    "    result = pd.DataFrame(prediction.compute())\n",\
    "    model['dask_client'].close()\n",\
    "    return pd.concat([df,result], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "%time print(apply(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    # TODO\n",\
    "    # model['dask_kmeans'].save_model(MODEL_DIRECTORY + name + '.json')\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "save(model,'dask_kmeans')"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def load(name):\n",\
    "    # TODO\n",\
    "    model = {}\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "loaded_model = load('dask_kmeans')\n",\
    "loaded_model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"model\": model}\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "summary(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

[Clustering with UMAP and DBSCAN:summary]
[Clustering with UMAP and DBSCAN:apply]
[Clustering with UMAP and DBSCAN:fit]
[Clustering with UMAP and DBSCAN]
runtime = base
description = Clustering with UMAP and DBSCAN
category = Clustering
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - UMAP"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains an example for UMAP that seamlessly interfaces with the Deep Learning Toolkit for Splunk.<br>\n",\
    "<a href=\"https://umap-learn.readthedocs.io/en/latest/api.html\">https://umap-learn.readthedocs.io/en/latest/api.html</a>"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 1,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import umap\n",\
    "# ...\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 2,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "numpy version: 1.18.1\n",\
      "pandas version: 1.0.1\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup diabetes.csv <br>\n",\
    "| fit MLTKContainer mode=stage algo=umap n_components=3 BMI age blood_pressure diabetes_pedigree glucose_concentration number_pregnant serum_insulin skin_thickness into app:diabetes_umap as umap"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 11,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 12,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "       number_pregnant  glucose_concentration  blood_pressure  skin_thickness  \\\n",\
      "count       768.000000             768.000000      768.000000      768.000000   \n",\
      "mean          3.845052             120.894531       69.105469       20.536458   \n",\
      "std           3.369578              31.972618       19.355807       15.952218   \n",\
      "min           0.000000               0.000000        0.000000        0.000000   \n",\
      "25%           1.000000              99.000000       62.000000        0.000000   \n",\
      "50%           3.000000             117.000000       72.000000       23.000000   \n",\
      "75%           6.000000             140.250000       80.000000       32.000000   \n",\
      "max          17.000000             199.000000      122.000000       99.000000   \n",\
      "\n",\
      "       serum_insulin         BMI  diabetes_pedigree         age    response  \n",\
      "count     768.000000  768.000000         768.000000  768.000000  768.000000  \n",\
      "mean       79.799479   31.992578           0.471876   33.240885    0.348958  \n",\
      "std       115.244002    7.884160           0.331329   11.760232    0.476951  \n",\
      "min         0.000000    0.000000           0.078000   21.000000    0.000000  \n",\
      "25%         0.000000   27.300000           0.243750   24.000000    0.000000  \n",\
      "50%        30.500000   32.000000           0.372500   29.000000    0.000000  \n",\
      "75%       127.250000   36.600000           0.626250   41.000000    1.000000  \n",\
      "max       846.000000   67.100000           2.420000   81.000000    1.000000  \n",\
      "{'options': {'params': {'mode': 'stage', 'algo': 'umap', 'n_components': '2'}, 'args': ['BMI', 'age', 'blood_pressure', 'diabetes_pedigree', 'glucose_concentration', 'number_pregnant', 'response', 'serum_insulin', 'skin_thickness'], 'feature_variables': ['BMI', 'age', 'blood_pressure', 'diabetes_pedigree', 'glucose_concentration', 'number_pregnant', 'response', 'serum_insulin', 'skin_thickness'], 'model_name': 'diabetes_umap', 'algo_name': 'MLTKContainer', 'mlspl_limits': {'disabled': False, 'handle_new_cat': 'default', 'max_distinct_cat_values': '1000', 'max_distinct_cat_values_for_classifiers': '1000', 'max_distinct_cat_values_for_scoring': '1000', 'max_fit_time': '6000', 'max_inputs': '100000000', 'max_memory_usage_mb': '4000', 'max_model_size_mb': '150', 'max_score_time': '6000', 'streaming_apply': '0', 'use_sampling': '1'}, 'kfold_cv': None}, 'feature_variables': ['BMI', 'age', 'blood_pressure', 'diabetes_pedigree', 'glucose_concentration', 'number_pregnant', 'response', 'serum_insulin', 'skin_thickness']}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "df, param = stage(\"diabetes_umap\")\n",\
    "print(df.describe())\n",\
    "print(param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 13,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    model = {}\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 17,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "{}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 18,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    # model.fit()\n",\
    "    info = {\"message\": \"model trained\"}\n",\
    "    return info"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 19,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "{'message': 'model trained'}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(fit(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 20,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    p = {\n",\
    "        \"n_neighbors\": 15,\n",\
    "        \"n_components\": 2\n",\
    "    }\n",\
    "    min_confidence = 0.0\n",\
    "    if 'options' in param:\n",\
    "        if 'params' in param['options']:\n",\
    "            for k in p.keys():\n",\
    "                if k in param['options']['params']:\n",\
    "                    p[k] = param['options']['params'][k]\n",\
    "    \n",\
    "    #reducer = umap.UMAP(random_state=42)\n",\
    "    reducer = umap.UMAP(\n",\
    "        random_state=42, \n",\
    "        n_neighbors=int(p['n_neighbors']),\n",\
    "        n_components=int(p['n_components'])\n",\
    "    )\n",\
    "\n",\
    "    embedding = reducer.fit_transform(X)\n",\
    "    result = pd.DataFrame(embedding).add_prefix(\"umap_\")\n",\
    "    return pd.concat([df,result], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 21,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "             0          1\n",\
      "0    15.441371   9.171429\n",\
      "1    12.556065   7.874014\n",\
      "2    15.691984  11.179146\n",\
      "3    -0.318175   4.113552\n",\
      "4    -3.303875   6.644968\n",\
      "..         ...        ...\n",\
      "763  -4.314896   7.316275\n",\
      "764  14.517242   7.850934\n",\
      "765  -1.093813   5.739352\n",\
      "766  13.944445  12.655777\n",\
      "767  12.764308   7.787235\n",\
      "\n",\
      "[768 rows x 2 columns]\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(apply(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def load(name):\n",\
    "    model = {}\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 5


[Named Entity Recognition and Extraction:fit]
[Named Entity Recognition and Extraction]
runtime = base
description = Named Entity Recognition using spaCy for NLP tasks
category = NLP
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit Notebook for Named Entity Recognition and Extraction with spaCy"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Splunk Machine Learning Toolkit (MLTK) Container for TensorFlow. This script contains an example of how to run an entity extraction algorithm over text using the spacy library."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import datetime\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import spacy\n",\
    "\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"spacy version: \" + spacy.__version__)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import sys\n",\
    "!{sys.executable} -m spacy download en_core_web_sm"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a prepared dataset into this environment."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| makeresults\n",\
    "| eval text = \"Boris Johnson has met Emmanuel Macron in Paris for Brexit talks, with the French president saying the UK's vote to quit the EU must be respected, but he added that the Ireland-Northern Ireland backstop plan was 'indispensable' to preserving political stability and the single market.;The backstop, opposed by Mr Johnson, aims to prevent a hard border on the island of Ireland after Brexit. Mr Johnson said that with 'energy and creativity we can find a way forward'.;On Wednesday German Chancellor Angela Merkel said the onus was on the UK to find a workable plan.;UK Prime Minister Mr Johnson insists the backstop must be ditched if a no-deal exit from the EU on 31 October is to be avoided.;He argues that it could leave the UK tied to the EU indefinitely, contrary to the result of the 2016 referendum, in which 52% of voters opted to leave.;But the EU has repeatedly said the withdrawal deal negotiated by former PM Theresa May, which includes the backstop, cannot be renegotiated.;However, it has previously said it would be willing to 'improve' the political declaration - the document that sets out the UK's future relationship with the EU.;Speaking after he greeted Mr Johnson at Paris's Elysee Palace, Mr Macron said he was 'very confident' that the UK and EU would be able to find a solution within 30 days - a timetable suggested by Mrs Merkel - 'if there is a good will on both sides'.;He said it would not be possible to find a new withdrawal agreement 'very different from the existing one' within that time, but added that an answer could be reached 'without reshuffling' the current deal.;Mr Macron also denied that he was the 'hard boy in the band', following suggestions that he would be tougher on the UK than his German counterpart.;Standing beside Mr Macron, Mr Johnson said he had been 'powerfully encouraged' by his conversations with Mrs Merkel in Berlin on Wednesday.;He emphasised his desire for a deal with the EU but added that it was 'vital for trust in politics' that the UK left the EU on 31 October.'He also said that 'under no circumstances' would the UK put checks or controls on the Ireland-UK border.;The two leaders ate lunch, drank coffee and walked through the Elysee gardens together during their talks, which lasted just under two hours. Mr Johnson then left to fly back to the UK.\"\n",\
    "| makemv text delim=\";\"\n",\
    "| mvexpand text\n",\
    "| fit MLTKContainer algo=spacy_ner epochs=100 text into app:spacy_entity_extraction_model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"spacy_entity_extraction_model in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",\
    "df, param = stage(\"spacy_entity_extraction_model\")\n",\
    "print(df)\n",\
    "print(df.shape)\n",\
    "print(str(param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize the model\n",\
    "# params: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    # Load English tokenizer, tagger, parser, NER and word vectors\n",\
    "    import en_core_web_sm\n",\
    "    model = en_core_web_sm.load()\n",\
    "    #model = spacy.load(\"en_core_web_sm\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "model = init(df,param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note that for this algorithm the model is pre-trained (the en_core_web_sm library comes pre-packaged by spacy) and therefore this stage is a placeholder only"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# returns a fit info json object\n",\
    "def fit(model,df,param):\n",\
    "    returns = {}\n",\
    "    \n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']].values.tolist()\n",\
    "    \n",\
    "    returns = list()\n",\
    "    \n",\
    "    for i in range(len(X)):\n",\
    "        doc = model(str(X[i]))\n",\
    "        \n",\
    "        \n",\
    "        entities = ''\n",\
    "    \n",\
    "        # Find named entities, phrases and concepts\n",\
    "        for entity in doc.ents:\n",\
    "            if entities == '':\n",\
    "                entities = entities + entity.text + ':' + entity.label_\n",\
    "            else:\n",\
    "                entities = entities + '|' + entity.text + ':' + entity.label_\n",\
    "        \n",\
    "        returns.append(entities)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = apply(model,df,param)\n",\
    "print(returns)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def save(model,name):\n",\
    "    # model will not be saved or reloaded as it is pre-built\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",\
    "def load(name):\n",\
    "    # model will not be saved or reloaded as it is pre-built\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"spacy\": spacy.__version__} }\n",\
    "    if model is not None:\n",\
    "        # Save keras model summary to string:\n",\
    "        s = []\n",\
    "        returns[\"summary\"] = ''.join(s)\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 2


[Graph Centrality Algorithms:fit]
[Graph Centrality Algorithms] 
runtime = base
description = Graph centrality algorithms using NetworkX
category = Graphs
source_code_version = 6
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Graph Algorithms with NetworkX"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains examples for graph algorithms available in NetworkX"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Note: By default every time you save this notebook the cells are exported into a python module which is then invoked by Splunk MLTK commands like <code> | fit ... | apply ... | summary </code>. Please read the Model Development Guide in the Deep Learning Toolkit app for more information."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import networkx as nx\n",\
    "# ...\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"networkx version: \" + nx.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup bitcoin_transactions.csv<br>\n",\
    "| head 1000<br>\n",\
    "| rename user_id_from as src user_id_to as dest<br>\n",\
    "| fit MLTKContainer mode=stage algo=graph_algo compute=\"eigenvector_centrality,cluster_coefficient,betweenness_centrality\" from src dest into app:bitcoin_graph as graph"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "df, param = stage(\"bitcoin_graph\")\n",\
    "print(df[0:1])\n",\
    "print(param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    model = nx.Graph()\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "model = init(df,param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    model.clear()\n",\
    "    src_dest_name = param['feature_variables']\n",\
    "    dfg = df[src_dest_name]\n",\
    "    for index, row in dfg.iterrows():\n",\
    "        model.add_edge(row[src_dest_name[0]], row[src_dest_name[1]]) #, value=row['value'])\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "g = fit(model,df,param)\n"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "param['options']['params']['compute'].lstrip(\"\\\"\").rstrip(\"\\\"\").lower().split(',')"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def apply(model,df,param):\n",\
    "    src_dest_name = param['feature_variables']\n",\
    "    algos = param['options']['params']['compute'].lstrip(\"\\\"\").rstrip(\"\\\"\").lower().split(',')\n",\
    "    outputcolumns = []\n",\
    "    results_df = df.copy()\n",\
    "    for algo in algos:\n",\
    "        if algo=='degree_centrality':\n",\
    "            cents = nx.algorithms.centrality.degree_centrality(model)\n",\
    "            outputcolumns.append(algo)\n",\
    "        elif algo=='betweenness_centrality':\n",\
    "            cents = nx.algorithms.centrality.betweenness_centrality(model)\n",\
    "            outputcolumns.append(algo)\n",\
    "        elif algo=='eigenvector_centrality':\n",\
    "            cents = nx.algorithms.centrality.eigenvector_centrality(model, max_iter=200)\n",\
    "            outputcolumns.append(algo)\n",\
    "        elif algo=='cluster_coefficient':\n",\
    "            cents = nx.algorithms.cluster.clustering(model)\n",\
    "            outputcolumns.append(algo)\n",\
    "        else:\n",\
    "            continue\n",\
    "        degs = pd.DataFrame(list(cents.items()), columns=[src_dest_name[0], algo])\n",\
    "        results_df = results_df.join(degs.set_index(src_dest_name[0]), on=src_dest_name[0])\n",\
    "    return pd.concat([df,results_df], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(apply(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    # with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",\
    "    #    json.dump(model, file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def load(name):\n",\
    "    model = init(None,None)\n",\
    "    # with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",\
    "    #    model = json.load(file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__, \"networkx\": nx.__version__} }\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python [conda env:root] *",\
   "language": "python",\
   "name": "conda-root-py"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\


[Correlation Matrix:summary]
[Correlation Matrix:fit]
[Correlation Matrix]
runtime = base
description = Correlation Matrix and Pair Plot
category = Basic
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Correlation Matrix and Pairplot Notebook"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 1,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import seaborn as sns\n",\
    "import matplotlib.pyplot as plt\n",\
    "# ...\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 2,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "numpy version: 1.18.1\n",\
      "pandas version: 1.0.1\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | compute command to do this."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup diabetes.csv</br>\n",\
    "| compute algorithm=\"Correlation Matrix\" environment=\"Docker\" mode=stage method=\"fit\" plot=\"matrix,pairplot\" model_name=\"diabetes_correlation\" fields=\"response,BMI,age,blood_pressure,diabetes_pedigree,glucose_concentration,number_pregnant,serum_insulin,skin_thickness\" feature_variables=\"response,BMI,age,blood_pressure,diabetes_pedigree,glucose_concentration,number_pregnant,serum_insulin,skin_thickness\""\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 3,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 4,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "         response         BMI         age  blood_pressure  diabetes_pedigree  \\\n",\
      "count  768.000000  768.000000  768.000000      768.000000         768.000000   \n",\
      "mean     0.348958   31.992578   33.240885       69.105469           0.471876   \n",\
      "std      0.476951    7.884160   11.760232       19.355807           0.331329   \n",\
      "min      0.000000    0.000000   21.000000        0.000000           0.078000   \n",\
      "25%      0.000000   27.300000   24.000000       62.000000           0.243750   \n",\
      "50%      0.000000   32.000000   29.000000       72.000000           0.372500   \n",\
      "75%      1.000000   36.600000   41.000000       80.000000           0.626250   \n",\
      "max      1.000000   67.100000   81.000000      122.000000           2.420000   \n",\
      "\n",\
      "       glucose_concentration  number_pregnant  serum_insulin  skin_thickness  \n",\
      "count             768.000000       768.000000     768.000000      768.000000  \n",\
      "mean              120.894531         3.845052      79.799479       20.536458  \n",\
      "std                31.972618         3.369578     115.244002       15.952218  \n",\
      "min                 0.000000         0.000000       0.000000        0.000000  \n",\
      "25%                99.000000         1.000000       0.000000        0.000000  \n",\
      "50%               117.000000         3.000000      30.500000       23.000000  \n",\
      "75%               140.250000         6.000000     127.250000       32.000000  \n",\
      "max               199.000000        17.000000     846.000000       99.000000  \n",\
      "{'options': {'params': {'algo': 'notebook', 'algorithm': 'Correlation Matrix', 'environment': 'Docker', 'mode': 'stage', 'method': 'fit', 'plot': 'matrix,pairplot', 'model_name': 'diabetes_correlation', 'fields': 'response,BMI,age,blood_pressure,diabetes_pedigree,glucose_concentration,number_pregnant,serum_insulin,skin_thickness', 'feature_variables': ['response', 'BMI', 'age', 'blood_pressure', 'diabetes_pedigree', 'glucose_concentration', 'number_pregnant', 'serum_insulin', 'skin_thickness']}, 'model_name': 'diabetes_correlation'}, 'feature_variables': ['response', 'BMI', 'age', 'blood_pressure', 'diabetes_pedigree', 'glucose_concentration', 'number_pregnant', 'serum_insulin', 'skin_thickness']}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "df, param = stage(\"diabetes_correlation\")\n",\
    "print(df.describe())\n",\
    "print(param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 7,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    model = {}\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 8,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "{}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 9,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    # model.fit()\n",\
    "    info = {\"message\": \"no fit needed\"}\n",\
    "    return info"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 10,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "{'message': 'no fit needed'}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(fit(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 11,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def plot_to_base64(plot):\n",\
    "    import base64\n",\
    "    import io \n",\
    "    pic_IObytes = io.BytesIO()\n",\
    "    if hasattr(plot,'fig'):\n",\
    "        plot.fig.savefig(pic_IObytes, format='png')\n",\
    "    elif hasattr(plot,'figure'):\n",\
    "        plot.figure.savefig(pic_IObytes, format='png')\n",\
    "    pic_IObytes.seek(0)\n",\
    "    pic_hash = base64.b64encode(pic_IObytes.read())\n",\
    "    return pic_hash\n",\
    "\n",\
    "\n",\
    "def plot_pairplot_as_base64(df,param):\n",\
    "    hue=None\n",\
    "    if 'options' in param:\n",\
    "        if 'target_variable' in param['options']:\n",\
    "            hue=str(param['options']['target_variable'][0])\n",\
    "    plot = sns.pairplot(df,hue=hue, palette=\"husl\")\n",\
    "    return str(plot_to_base64(plot))\n",\
    "\n",\
    "\n",\
    "def plot_correlationmatrix_as_base64(corr):\n",\
    "    # Set up the matplotlib figure\n",\
    "    f, ax = plt.subplots(figsize=(15, 15))\n",\
    "    # Generate a mask for the upper triangle\n",\
    "    mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",\
    "    # Generate a custom diverging colormap\n",\
    "    cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",\
    "    # Draw the heatmap with the mask and correct aspect ratio\n",\
    "    #plot = sns.heatmap(corr, mask=mask, cmap=\"Spectral\", vmax=1.0, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",\
    "    plot = sns.heatmap(corr, cmap=\"Spectral\", vmax=1.0, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",\
    "    #plot.figure.savefig(\"plot.png\", format='png')\n",\
    "    return str(plot_to_base64(plot))\n",\
    "\n",\
    "def apply(model,df,param):\n",\
    "    # param['options']['model_name']    \n",\
    "    dfeatures = df[param['feature_variables']]\n",\
    "    result = dfeatures.corr() #.reset_index()\n",\
    "    if 'plot' in param['options']['params']:\n",\
    "        plots = param['options']['params']['plot'].lstrip(\"\\\"\").rstrip(\"\\\"\").lower().split(',')\n",\
    "        for plot in plots:\n",\
    "            if plot=='matrix':\n",\
    "                model[\"plot_matrix\"] = plot_correlationmatrix_as_base64(result)\n",\
    "            elif plot=='pairplot':\n",\
    "                model[\"plot_pairplot\"] = plot_pairplot_as_base64(df,param)\n",\
    "            else:\n",\
    "                continue\n",\
    "\n",\
    "    return result\n"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "apply(model,df,param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 13,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",\
    "        json.dump(model, file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 15,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "model  = save(model,'correlationmatrix_diabetes_correlation')"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 16,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def load(name):\n",\
    "    model = {}\n",\
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",\
    "        model = json.load(file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 17,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "model = load('correlationmatrix_diabetes_correlation')"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 18,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 20,\
   "metadata": {},\
   "outputs": [\
    {\
     "data": {\
      "text/plain": [\
       "{'version': {'numpy': '1.18.1', 'pandas': '1.0.1'}}"\
      ]\
     },\
     "execution_count": 20,\
     "metadata": {},\
     "output_type": "execute_result"\
    }\
   ],\
   "source": [\
    "summary(model)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 12


[Distributed Random Forest Regressor:fit]
[Distributed Random Forest Regressor:fit_only]
[Distributed Random Forest Regressor:fit_and_score]
[Distributed Random Forest Regressor]
description = Distributed Random Forest using Spark RDD MLlib
category = Classifier
runtime = spark
source_code = {\
 "cells": [\
  {\
   "cell_type": "code",\
   "execution_count": 45,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "import logging\n",\
    "import io\n",\
    "import json\n",\
    "import numpy as np\n",\
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",\
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",\
    "from pyspark.mllib.util import MLUtils\n",\
    "from pyspark.mllib.regression import LabeledPoint\n",\
    "from pyspark.mllib.linalg import Vectors\n",\
    "from pyspark.mllib.evaluation import RegressionMetrics\n",\
    "\n",\
    "def fit_only(sc, events):\n",\
    "    # Step 1: Prepare data as labeled points\n",\
    "    training_data = events.map(lambda row: LabeledPoint(float(row[\"fare_amount\"]), [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "    \n",\
    "    # Step 2: Train model using Random Forest Regressor\n",\
    "    model = RandomForest.trainRegressor(\n",\
    "        training_data,\n",\
    "        categoricalFeaturesInfo={},\n",\
    "        numTrees=10, \n",\
    "        featureSubsetStrategy=\"auto\",\n",\
    "        impurity=\"variance\",\n",\
    "        maxDepth=4,\n",\
    "        maxBins=32\n",\
    "    )\n",\
    "    return [{\"status\": \"success\"}]\n",\
    "\n",\
    "\n",\
    "\n",\
    "\n",\
    "def fit(sc, events):\n",\
    "    \n",\
    "    # Step 1: Prepare data as labeled points\n",\
    "    training_data = events.map(lambda row: LabeledPoint(float(row[\"fare_amount\"]), [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "    \n",\
    "    # Step 2: Train model using Random Forest Regressor\n",\
    "    model = RandomForest.trainRegressor(\n",\
    "        training_data,\n",\
    "        categoricalFeaturesInfo={},                        \n",\
    "        numTrees=10, \n",\
    "        featureSubsetStrategy=\"auto\",\n",\
    "        impurity=\"variance\",\n",\
    "        maxDepth=4,\n",\
    "        maxBins=32\n",\
    "    )\n",\
    "\n",\
    "    # Step 3: Compute predictions\n",\
    "    predictions = model.predict(events.map(lambda row: [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "\n",\
    "    # Step 4: Join predictions and return results\n",\
    "    results = events.zip(predictions) \n",\
    "    def format_results(t):\n",\
    "        e,p=t\n",\
    "        e[\"predicted_fare_amount\"]=round(p,1)\n",\
    "        return e\n",\
    "    return results.map(format_results)\n",\
    "\n",\
    "\n",\
    "\n",\
    "def fit_and_score(sc, events):\n",\
    "    rdd = events.map(lambda row: LabeledPoint(float(row[\"fare_amount\"]), [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "    model = RandomForest.trainRegressor(rdd, categoricalFeaturesInfo={},\n",\
    "                                numTrees=10, featureSubsetStrategy=\"auto\",\n",\
    "                                impurity=\"variance\", maxDepth=4, maxBins=32)    \n",\
    "\n",\
    "    predictions = model.predict(events.map(lambda row: [\n",\
    "        float(row[\"DOLocationID\"]),\n",\
    "        float(row[\"PULocationID\"])\n",\
    "    ]))\n",\
    "\n",\
    "    # Join prediction with input events\n",\
    "    results = events.zip(predictions) \n",\
    "    def format_results(t):\n",\
    "        e,p=t\n",\
    "        e[\"predicted_fare_amount\"]=p\n",\
    "        return e\n",\
    "    results = results.map(format_results)\n",\
    "    \n",\
    "\n",\
    "    # Calculate Scoring Metrics\n",\
    "    values_and_predictions = results.map(lambda p: (float(p[\"predicted_fare_amount\"]), float(p[\"fare_amount\"])))\n",\
    "\n",\
    "    metrics = RegressionMetrics(values_and_predictions)\n",\
    "    return [{\"MSE\": metrics.meanSquaredError, \n",\
    "             \"RMSE\": metrics.rootMeanSquaredError, \n",\
    "             \"R2\": metrics.r2, \n",\
    "             \"MAE\": metrics.meanAbsoluteError, \n",\
    "             \"Explained variance\": metrics.explainedVariance, \n",\
    "            }]"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.3"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 1\
}\

source_code_version = 35

[XGBoost Classifier:summary]
[XGBoost Classifier:fit]
[XGBoost Classifier]
runtime = base
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit for Splunk - Example for SHAP with XGBOOST"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Deep Learning Toolkit for Splunk."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import xgboost\n",\
    "import shap\n",\
    "import matplotlib.pyplot as plt\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"xgboost version: \" + xgboost.__version__)\n",\
    "print(\"shap version: \" + shap.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | fit command to do this."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup diabetes.csv</br>\n",\
    "| compute algorithm=\"XGBoost Classifier\" environment=\"Docker\" method=\"fit\" mode=\"stage\" plot=\"violin\" target_variables=\"response\" feature_variables=\"BMI,age,blood_pressure,diabetes_pedigree,glucose_concentration,number_pregnant,serum_insulin,skin_thickness\" fields=\"response,BMI,age,blood_pressure,diabetes_pedigree,glucose_concentration,number_pregnant,serum_insulin,skin_thickness\" model_name=\"diabetes_xgboost_classifier\""\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"barebone_model\" in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "df, param = stage(\"diabetes_xgboost_classifier\")\n",\
    "print(df.describe())\n",\
    "print(param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    model = {}\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    y = df[param['target_variables']] \n",\
    "    learning_rate = 0.01\n",\
    "    if 'learning_rate' in param['options']['params']:\n",\
    "        learning_rate = float(param['options']['params']['learning_rate'].lstrip(\"\\\"\").rstrip(\"\\\"\"))\n",\
    "    model['xgboost'] = xgboost.train({\"learning_rate\": 0.01}, xgboost.DMatrix(X, label=y), 100)\n",\
    "    # explain the model's prediction using SHAP values\n",\
    "    model['shap_values'] = shap.TreeExplainer(model['xgboost']).shap_values(X)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(fit(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def plot_to_base64(plot):\n",\
    "    import base64\n",\
    "    import io \n",\
    "    pic_IObytes = io.BytesIO()\n",\
    "    plot.savefig(pic_IObytes, format='png')\n",\
    "    pic_IObytes.seek(0)\n",\
    "    pic_hash = base64.b64encode(pic_IObytes.read())\n",\
    "    return pic_hash\n",\
    "\n",\
    "def apply(model,df,param):\n",\
    "    X = df[param['feature_variables']]\n",\
    "    result = model['xgboost'].predict(xgboost.DMatrix(X))\n",\
    "    if 'plot' in param['options']['params']:\n",\
    "        plots = param['options']['params']['plot'].lstrip(\"\\\"\").rstrip(\"\\\"\").lower().split(',')\n",\
    "        if 'shap_values' in model:            \n",\
    "            shap_values = model['shap_values']\n",\
    "            plt.clf()\n",\
    "            for plot in plots:\n",\
    "                print(plot)\n",\
    "                if plot=='violin':\n",\
    "                    shap.summary_plot(shap_values, X, show=False, plot_type=\"violin\")\n",\
    "                elif plot=='layered_violin':\n",\
    "                    shap.summary_plot(shap_values, X, show=False, plot_type=\"layered_violin\", color='coolwarm')\n",\
    "                elif plot=='bar':\n",\
    "                    shap.summary_plot(shap_values, X, show=False, plot_type=\"bar\")\n",\
    "                else:\n",\
    "                    shap.summary_plot(shap_values, X, show=False)\n",\
    "                # export current plot\n",\
    "                plt.gcf().set_size_inches(10,4)\n",\
    "                plt.tight_layout()\n",\
    "                model[\"plot_shap\"] = plot_to_base64(plt)\n",\
    "    return pd.concat([df,pd.DataFrame(result).add_prefix(\"prediction_\")], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "apply(model,df,param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    # TODO if needed\n",\
    "    #with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",\
    "    #    json.dump(model, file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "model = save(model,'diabetes_shap_xgboost')"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def load(name):\n",\
    "    model = {}\n",\
    "    # TODO if needed\n",\
    "    # with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",\
    "    #    model = json.load(file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "model = load('diabetes_shap_xgboost')"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"xgboost\": xgboost.__version__, \"shap\": shap.__version__} }\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 12
deployment_code_version = 1
description = Explainable Machine Learning with XGBoost and SHAP
category = Classifier

[XGBoost Regressor:apply]
[XGBoost Regressor:fit]
[XGBoost Regressor]
runtime = base
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# XGBoost Regressor Example"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_import"\
   },\
   "outputs": [],\
   "source": [\
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",\
    "import json\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "from sklearn.model_selection import train_test_split\n",\
    "from sklearn.metrics import mean_absolute_error\n",\
    "from xgboost import XGBRegressor\n",\
    "# ...\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup server_power.csv <br>\n",\
    "| fields - total-cpu-utilization <br>\n",\
    "| compute algorithm=\"XGBoost Regressor\" environment=\"Docker\" method=\"fit\" mode=\"stage\" target_variables=\"ac_power\" feature_variables=\"total-disk-accesses,total-disk-blocks,total-disk-utilization,total-instructions_retired,total-last_level_cache_references,total-memory_bus_transactions,total-unhalted_core_cycles\" fields=\"ac_power,total-disk-accesses,total-disk-blocks,total-disk-utilization,total-instructions_retired,total-last_level_cache_references,total-memory_bus_transactions,total-unhalted_core_cycles\" model_name=\"server_power_xgboost_regressor\"<br>"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "df, param = stage(\"server_power_xgboost_regressor\")\n",\
    "print(df[0:1])\n",\
    "print(df.shape)\n",\
    "print(str(param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    #model = {}\n",\
    "    model = XGBRegressor()\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [],\
   "source": [\
    "# test mltkc_stage_create_model\n",\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    returns = {}\n",\
    "    X = df[param['feature_variables']]\n",\
    "    y = df[param['target_variables']]\n",\
    "    #train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3)\n",\
    "    #model.fit(train_X, train_y, verbose=False)\n",\
    "    #predictions = model.predict(test_X)\n",\
    "    #returns['Mean_Absolute_Error'] = str(mean_absolute_error(predictions, test_y))\n",\
    "    \n",\
    "    model.fit(X, y, verbose=False)\n",\
    "    \n",\
    "    info = {\"message\": \"model trained\"}\n",\
    "    return info"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = fit(model,df,param)\n",\
    "print(returns)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def apply(model,df,param):\n",\
    "    \n",\
    "    X = df[param['feature_variables']]    \n",\
    "    y_hat = model.predict(X)\n",\
    "    result = pd.DataFrame(y_hat, columns=['predicted_'+param['target_variables'][0]])\n",\
    "    \n",\
    "    return pd.concat([df,result], axis=1)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "returns = apply(model,df,param)\n",\
    "print(returns)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    path = MODEL_DIRECTORY + name + \".json\"\n",\
    "    model.save_model(path) \n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"<algo_name>_<model_name>\"\n",\
    "def load(name):\n",\
    "    model = XGBRegressor()\n",\
    "    model.load_model(MODEL_DIRECTORY + name + \".json\")\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "mltkc_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",\
    "    return returns"\
   ]\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}
source_code_version = 12
deployment_code_version = 1
description = Simple regression example with XGBoost
category = Regressor

[Causal Inference:fit]
[Causal Inference]
runtime = base
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit 4.x for Splunk - Causal Inference with Causalnex"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 3,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_import"\
   },\
   "outputs": [],\
   "source": [\
    "import json\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import causalnex\n",\
    "from causalnex.structure import DAGRegressor\n",\
    "from sklearn.model_selection import cross_val_score\n",\
    "from sklearn.preprocessing import StandardScaler\n",\
    "from sklearn.model_selection import KFold\n",\
    "# ...\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 4,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "numpy version: 1.18.1\n",\
      "pandas version: 0.25.3\n",\
      "causalnex version: 0.8.1\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"causalnex version: \" + causalnex.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | compute command to do this."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "| inputlookup housing.csv<br>\n",\
    "| compute algorithm=\"Causal Inference\" environment=\"Docker\" method=\"fit\" mode=\"stage\" fields=\"avg_rooms_per_dwelling,business_acres,charles_river_adjacency,crime_rate,distance_to_employment_center,highway_accessibility_index,land_zone,median_house_value,nitric_oxide_concentration,property_tax_rate,pupil_teacher_ratio,units_prior_1940\" target_variables=\"median_house_value\" feature_variables=\"avg_rooms_per_dwelling,business_acres,charles_river_adjacency,crime_rate,distance_to_employment_center,highway_accessibility_index,land_zone,nitric_oxide_concentration,property_tax_rate,pupil_teacher_ratio,units_prior_1940\" model_name=\"causalnex\" model_name=\"causalnex\"<br>\n"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 5,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed and should only be used for staging data into the notebook environment to have it accessible in this notebook\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 6,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "       avg_rooms_per_dwelling  business_acres  charles_river_adjacency  \\\n",\
      "count              506.000000      506.000000               506.000000   \n",\
      "mean                 6.284634       11.136779                 0.069170   \n",\
      "std                  0.702617        6.860353                 0.253994   \n",\
      "min                  3.561000        0.460000                 0.000000   \n",\
      "25%                  5.885500        5.190000                 0.000000   \n",\
      "50%                  6.208500        9.690000                 0.000000   \n",\
      "75%                  6.623500       18.100000                 0.000000   \n",\
      "max                  8.780000       27.740000                 1.000000   \n",\
      "\n",\
      "       crime_rate  distance_to_employment_center  highway_accessibility_index  \\\n",\
      "count  506.000000                     506.000000                   506.000000   \n",\
      "mean     3.613524                       3.795043                     9.549407   \n",\
      "std      8.601545                       2.105710                     8.707259   \n",\
      "min      0.006320                       1.129600                     1.000000   \n",\
      "25%      0.082045                       2.100175                     4.000000   \n",\
      "50%      0.256510                       3.207450                     5.000000   \n",\
      "75%      3.677083                       5.188425                    24.000000   \n",\
      "max     88.976200                      12.126500                    24.000000   \n",\
      "\n",\
      "        land_zone  median_house_value  nitric_oxide_concentration  \\\n",\
      "count  506.000000          506.000000                  506.000000   \n",\
      "mean    11.363636           22.532806                    0.554695   \n",\
      "std     23.322453            9.197104                    0.115878   \n",\
      "min      0.000000            5.000000                    0.385000   \n",\
      "25%      0.000000           17.025000                    0.449000   \n",\
      "50%      0.000000           21.200000                    0.538000   \n",\
      "75%     12.500000           25.000000                    0.624000   \n",\
      "max    100.000000           50.000000                    0.871000   \n",\
      "\n",\
      "       property_tax_rate  pupil_teacher_ratio  units_prior_1940  \n",\
      "count         506.000000           506.000000        506.000000  \n",\
      "mean          408.237154            18.455534         68.574901  \n",\
      "std           168.537116             2.164946         28.148861  \n",\
      "min           187.000000            12.600000          2.900000  \n",\
      "25%           279.000000            17.400000         45.025000  \n",\
      "50%           330.000000            19.050000         77.500000  \n",\
      "75%           666.000000            20.200000         94.075000  \n",\
      "max           711.000000            22.000000        100.000000  \n",\
      "{'options': {'params': {'algo': 'notebook', 'algorithm': 'Causal Inference', 'environment': 'Docker', 'method': 'fit', 'mode': 'stage', 'fields': 'avg_rooms_per_dwelling,business_acres,charles_river_adjacency,crime_rate,distance_to_employment_center,highway_accessibility_index,land_zone,median_house_value,nitric_oxide_concentration,property_tax_rate,pupil_teacher_ratio,units_prior_1940', 'target_variables': ['median_house_value'], 'feature_variables': ['avg_rooms_per_dwelling', 'business_acres', 'charles_river_adjacency', 'crime_rate', 'distance_to_employment_center', 'highway_accessibility_index', 'land_zone', 'nitric_oxide_concentration', 'property_tax_rate', 'pupil_teacher_ratio', 'units_prior_1940'], 'model_name': 'causalnex'}, 'model_name': 'causalnex'}, 'target_variables': ['median_house_value'], 'feature_variables': ['avg_rooms_per_dwelling', 'business_acres', 'charles_river_adjacency', 'crime_rate', 'distance_to_employment_center', 'highway_accessibility_index', 'land_zone', 'nitric_oxide_concentration', 'property_tax_rate', 'pupil_teacher_ratio', 'units_prior_1940']}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "df, param = stage(\"causalnex\")\n",\
    "print(df.describe())\n",\
    "print(param)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 7,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    model = DAGRegressor(\n",\
    "                alpha=0.1,\n",\
    "                beta=0.9,\n",\
    "                fit_intercept=True,\n",\
    "                hidden_layer_units=None,\n",\
    "                dependent_target=True,\n",\
    "                enforce_dag=True,\n",\
    "                 )\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 8,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "DAGRegressor(alpha=0.1, beta=0.9, dependent_target=True, dist_type_schema=None,\n",\
      "             enforce_dag=True, fit_intercept=True, hidden_layer_units=None,\n",\
      "             standardize=False, tabu_child_nodes=None, tabu_edges=None,\n",\
      "             tabu_parent_nodes=None, threshold=0.0)\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 9,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    \n",\
    "    target=param['target_variables'][0]\n",\
    "    \n",\
    "    #Data prep for processing\n",\
    "    y_p = df[target]\n",\
    "    y = y_p.values\n",\
    "\n",\
    "    X_p = df[param['feature_variables']]\n",\
    "    X = X_p.to_numpy()\n",\
    "    X_col = list(X_p.columns)\n",\
    "\n",\
    "    #Scale the data\n",\
    "    ss = StandardScaler()\n",\
    "    X_ss = ss.fit_transform(X)\n",\
    "    y_ss = (y - y.mean()) / y.std()\n",\
    "    \n",\
    "    scores = cross_val_score(model, X_ss, y_ss, cv=KFold(shuffle=True, random_state=42))\n",\
    "    print(f'MEAN R2: {np.mean(scores).mean():.3f}')\n",\
    "\n",\
    "    X_pd = pd.DataFrame(X_ss, columns=X_col)\n",\
    "    y_pd = pd.Series(y_ss, name=target)\n",\
    "\n",\
    "    model.fit(X_pd, y_pd)\n",\
    "    \n",\
    "    info = pd.Series(model.coef_, index=X_col)\n",\
    "    #info = pd.Series(model.coef_, index=list(df.drop(['_time'],axis=1).columns))\n",\
    "    return info"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 10,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stderr",\
     "output_type": "stream",\
     "text": [\
      "/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",\
      "  allow_unreachable=True)  # allow_unreachable flag\n"\
     ]\
    },\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "MEAN R2: 0.643\n",\
      "avg_rooms_per_dwelling           0.45491\n",\
      "business_acres                   0.00000\n",\
      "charles_river_adjacency          0.00000\n",\
      "crime_rate                       0.00000\n",\
      "distance_to_employment_center    0.00000\n",\
      "highway_accessibility_index      0.00000\n",\
      "land_zone                        0.00000\n",\
      "nitric_oxide_concentration       0.00000\n",\
      "property_tax_rate                0.00000\n",\
      "pupil_teacher_ratio              0.00000\n",\
      "units_prior_1940                 0.00000\n",\
      "dtype: float64\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(fit(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 11,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def apply(model,df,param):\n",\
    "    data = []\n",\
    "\n",\
    "    for col in list(df.columns):\n",\
    "        s = model.get_edges_to_node(col)\n",\
    "        for i in s.index:\n",\
    "            data.append([i,col,s[i]]);\n",\
    "\n",\
    "    graph = pd.DataFrame(data, columns=['src','dest','weight'])\n",\
    "\n",\
    "    #results to send back to Splunk\n",\
    "    graph_output=graph[graph['weight']>0]\n",\
    "    return graph_output"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 12,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "                               src                           dest    weight\n",\
      "19               property_tax_rate                 business_acres  0.298184\n",\
      "38     highway_accessibility_index                     crime_rate  0.415314\n",\
      "54                units_prior_1940  distance_to_employment_center  0.423097\n",\
      "63               property_tax_rate    highway_accessibility_index  0.758038\n",\
      "70   distance_to_employment_center                      land_zone  0.449486\n",\
      "77          avg_rooms_per_dwelling             median_house_value  0.454910\n",\
      "89                  business_acres     nitric_oxide_concentration  0.377974\n",\
      "115    highway_accessibility_index            pupil_teacher_ratio  0.282895\n",\
      "116                      land_zone            pupil_teacher_ratio  0.291228\n",\
      "117     nitric_oxide_concentration            pupil_teacher_ratio  0.303311\n",\
      "128     nitric_oxide_concentration               units_prior_1940  0.496030\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(apply(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 14,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "# install graphviz to display inline\n",\
    "#model.plot_dag(True)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"algo_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    #with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",\
    "    #    json.dump(model, file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "saved_model = save(model,'algo_barebone_model')\n",\
    "saved_model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"algo_<model_name>\"\n",\
    "def load(name):\n",\
    "    model = {}\n",\
    "    #with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",\
    "    #    model = json.load(file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "loaded_model = load('algo_barebone_model')\n",\
    "loaded_model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "summary(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Finally you can reuse the model by calling your SPL with | compute ... method=\"apply\""\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

source_code_version = 7
deployment_code_version = 1
description = Causal inference with causalnex
category = Graphs

[Process Miner:fit]
[Process Miner:summary]
[Process Miner]
description = Process Mining with PM4Py
category = Data Mining
source_code_version = 94
source_code = {\
 "cells": [\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "# Deep Learning Toolkit 4.x for Splunk - Process Mining with PM4Py"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "This notebook contains a barebone example workflow how to work on custom containerized code that seamlessly runs in Splunk Enterprise and interfaces with the Deep Learning Toolkit for Splunk."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 0 - import libraries\n",\
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 1,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_import"\
   },\
   "outputs": [],\
   "source": [\
    "import json\n",\
    "import numpy as np\n",\
    "import pandas as pd\n",\
    "import pm4py\n",\
    "from pm4py.objects.log.util import dataframe_utils\n",\
    "from pm4py.objects.conversion.log import converter as log_converter\n",\
    "from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n",\
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",\
    "from pm4py.algo.discovery.dfg import algorithm as dfg_discovery\n",\
    "# ...\n",\
    "# global constants\n",\
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 2,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "numpy version: 1.18.1\n",\
      "pandas version: 0.25.3\n",\
      "pm4py version: 2.0.1.3\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(\"numpy version: \" + np.__version__)\n",\
    "print(\"pandas version: \" + pd.__version__)\n",\
    "print(\"pm4py version: \" + pm4py.__version__)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 1 - get a data sample from Splunk\n",\
    "In Splunk run a search to pipe a dataset into your notebook environment. Note: mode=stage is used in the | compute command to do this."\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "index=_internal uri=* user=* <br>\n",\
    "| stats count by _time uri user <br>\n",\
    "| eval start_timestamp=strftime(_time, \"%Y%m%dT%H%M%S\") <br>\n",\
    "| rename uri as case:concept:name user as concept:name <br>\n",\
    "| eval time:timestamp = start_timestamp<br>\n",\
    "| compute algorithm=\"Process Miner\" environment=\"Docker\" method=\"fit\" mode=\"stage\" fields=\"case:concept:name,concept:name,start_timestamp,time:timestamp\"<br>"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the model_name value or set to \"default\" if no model_name is present. This step is intended to work with a subset of your data to create your custom model."\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 3,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_stage"\
   },\
   "outputs": [],\
   "source": [\
    "# this cell is not executed and should only be used for staging data into the notebook environment to have it accessible in this notebook\n",\
    "def stage(name):\n",\
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",\
    "        df = pd.read_csv(f)\n",\
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",\
    "        param = json.load(f)\n",\
    "    return df, param"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 7,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "{'options': {'params': {'algo': 'notebook', 'algorithm': 'Process Miner', 'environment': 'Docker', 'method': 'fit', 'mode': 'stage', 'fields': 'case:concept:name,concept:name,start_timestamp,time:timestamp'}}}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "df, param = stage(\"default\")\n",\
    "print(param)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 8,\
   "metadata": {},\
   "outputs": [\
    {\
     "data": {\
      "text/html": [\
       "<div>\n",\
       "<style scoped>\n",\
       "    .dataframe tbody tr th:only-of-type {\n",\
       "        vertical-align: middle;\n",\
       "    }\n",\
       "\n",\
       "    .dataframe tbody tr th {\n",\
       "        vertical-align: top;\n",\
       "    }\n",\
       "\n",\
       "    .dataframe thead th {\n",\
       "        text-align: right;\n",\
       "    }\n",\
       "</style>\n",\
       "<table border=\"1\" class=\"dataframe\">\n",\
       "  <thead>\n",\
       "    <tr style=\"text-align: right;\">\n",\
       "      <th></th>\n",\
       "      <th>case:concept:name</th>\n",\
       "      <th>concept:name</th>\n",\
       "      <th>start_timestamp</th>\n",\
       "      <th>time:timestamp</th>\n",\
       "    </tr>\n",\
       "  </thead>\n",\
       "  <tbody>\n",\
       "    <tr>\n",\
       "      <th>0</th>\n",\
       "      <td>/en-GB/splunkd/__raw/services/server/health/sp...</td>\n",\
       "      <td>admin</td>\n",\
       "      <td>20201130T181257</td>\n",\
       "      <td>20201130T181257</td>\n",\
       "    </tr>\n",\
       "    <tr>\n",\
       "      <th>1</th>\n",\
       "      <td>/en-GB/splunkd/__raw/services/server/health/sp...</td>\n",\
       "      <td>admin</td>\n",\
       "      <td>20201130T181259</td>\n",\
       "      <td>20201130T181259</td>\n",\
       "    </tr>\n",\
       "    <tr>\n",\
       "      <th>2</th>\n",\
       "      <td>/servicesNS/nobody/deep-learning-toolkit/prope...</td>\n",\
       "      <td>splunk-system-user</td>\n",\
       "      <td>20201130T181301</td>\n",\
       "      <td>20201130T181301</td>\n",\
       "    </tr>\n",\
       "    <tr>\n",\
       "      <th>3</th>\n",\
       "      <td>/servicesNS/nobody/deep-learning-toolkit/confi...</td>\n",\
       "      <td>splunk-system-user</td>\n",\
       "      <td>20201130T181301</td>\n",\
       "      <td>20201130T181301</td>\n",\
       "    </tr>\n",\
       "    <tr>\n",\
       "      <th>4</th>\n",\
       "      <td>/servicesNS/nobody/deep-learning-toolkit/saved...</td>\n",\
       "      <td>splunk-system-user</td>\n",\
       "      <td>20201130T181301</td>\n",\
       "      <td>20201130T181301</td>\n",\
       "    </tr>\n",\
       "    <tr>\n",\
       "      <th>...</th>\n",\
       "      <td>...</td>\n",\
       "      <td>...</td>\n",\
       "      <td>...</td>\n",\
       "      <td>...</td>\n",\
       "    </tr>\n",\
       "    <tr>\n",\
       "      <th>6309</th>\n",\
       "      <td>/servicesNS/nobody/deep-learning-toolkit/prope...</td>\n",\
       "      <td>splunk-system-user</td>\n",\
       "      <td>20201130T182755</td>\n",\
       "      <td>20201130T182755</td>\n",\
       "    </tr>\n",\
       "    <tr>\n",\
       "      <th>6310</th>\n",\
       "      <td>/servicesNS/nobody/deep-learning-toolkit/confi...</td>\n",\
       "      <td>splunk-system-user</td>\n",\
       "      <td>20201130T182755</td>\n",\
       "      <td>20201130T182755</td>\n",\
       "    </tr>\n",\
       "    <tr>\n",\
       "      <th>6311</th>\n",\
       "      <td>/servicesNS/nobody/deep-learning-toolkit/prope...</td>\n",\
       "      <td>splunk-system-user</td>\n",\
       "      <td>20201130T182755</td>\n",\
       "      <td>20201130T182755</td>\n",\
       "    </tr>\n",\
       "    <tr>\n",\
       "      <th>6312</th>\n",\
       "      <td>/servicesNS/nobody/deep-learning-toolkit/confi...</td>\n",\
       "      <td>splunk-system-user</td>\n",\
       "      <td>20201130T182755</td>\n",\
       "      <td>20201130T182755</td>\n",\
       "    </tr>\n",\
       "    <tr>\n",\
       "      <th>6313</th>\n",\
       "      <td>/servicesNS/nobody/deep-learning-toolkit/prope...</td>\n",\
       "      <td>splunk-system-user</td>\n",\
       "      <td>20201130T182755</td>\n",\
       "      <td>20201130T182755</td>\n",\
       "    </tr>\n",\
       "  </tbody>\n",\
       "</table>\n",\
       "<p>6314 rows × 4 columns</p>\n",\
       "</div>"\
      ],\
      "text/plain": [\
       "                                      case:concept:name        concept:name  \\\n",\
       "0     /en-GB/splunkd/__raw/services/server/health/sp...               admin   \n",\
       "1     /en-GB/splunkd/__raw/services/server/health/sp...               admin   \n",\
       "2     /servicesNS/nobody/deep-learning-toolkit/prope...  splunk-system-user   \n",\
       "3     /servicesNS/nobody/deep-learning-toolkit/confi...  splunk-system-user   \n",\
       "4     /servicesNS/nobody/deep-learning-toolkit/saved...  splunk-system-user   \n",\
       "...                                                 ...                 ...   \n",\
       "6309  /servicesNS/nobody/deep-learning-toolkit/prope...  splunk-system-user   \n",\
       "6310  /servicesNS/nobody/deep-learning-toolkit/confi...  splunk-system-user   \n",\
       "6311  /servicesNS/nobody/deep-learning-toolkit/prope...  splunk-system-user   \n",\
       "6312  /servicesNS/nobody/deep-learning-toolkit/confi...  splunk-system-user   \n",\
       "6313  /servicesNS/nobody/deep-learning-toolkit/prope...  splunk-system-user   \n",\
       "\n",\
       "      start_timestamp   time:timestamp  \n",\
       "0     20201130T181257  20201130T181257  \n",\
       "1     20201130T181259  20201130T181259  \n",\
       "2     20201130T181301  20201130T181301  \n",\
       "3     20201130T181301  20201130T181301  \n",\
       "4     20201130T181301  20201130T181301  \n",\
       "...               ...              ...  \n",\
       "6309  20201130T182755  20201130T182755  \n",\
       "6310  20201130T182755  20201130T182755  \n",\
       "6311  20201130T182755  20201130T182755  \n",\
       "6312  20201130T182755  20201130T182755  \n",\
       "6313  20201130T182755  20201130T182755  \n",\
       "\n",\
       "[6314 rows x 4 columns]"\
      ]\
     },\
     "execution_count": 8,\
     "metadata": {},\
     "output_type": "execute_result"\
    }\
   ],\
   "source": [\
    "df"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 2 - create and initialize a model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 9,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_init"\
   },\
   "outputs": [],\
   "source": [\
    "# initialize your model\n",\
    "# available inputs: data and parameters\n",\
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",\
    "def init(df,param):\n",\
    "    model = {}\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 10,\
   "metadata": {\
    "scrolled": true\
   },\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "{}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "model = init(df,param)\n",\
    "print(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 3 - fit the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 11,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_fit"\
   },\
   "outputs": [],\
   "source": [\
    "# train your model\n",\
    "# returns a fit info json object and may modify the model object\n",\
    "def fit(model,df,param):\n",\
    "    info = {}\n",\
    "    return info"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 12,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "{}\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "print(fit(model,df,param))"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 4 - apply the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 56,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_apply"\
   },\
   "outputs": [],\
   "source": [\
    "# apply your model\n",\
    "# returns the calculated results\n",\
    "def pngfile_to_base64(filepath):\n",\
    "    import base64\n",\
    "    import io\n",\
    "    with open(filepath, 'rb') as file:\n",\
    "        pic_hash = base64.b64encode(file.read())\n",\
    "    return str(pic_hash)\n",\
    "\n",\
    "def apply(model,df,param):\n",\
    "    # convert dataframe to pm4py compatible event_log object\n",\
    "    log_csv = dataframe_utils.convert_timestamp_columns_in_df(df)\n",\
    "    log_csv = log_csv.sort_values('start_timestamp')\n",\
    "    event_logs = log_converter.apply(log_csv, variant=log_converter.Variants.TO_EVENT_LOG)\n",\
    "\n",\
    "    # apply dfg discovery\n",\
    "    dfg, start_activities, end_activities = pm4py.discover_dfg(event_logs)\n",\
    "    #dfg = dfg_discovery.apply(event_logs, variant=dfg_discovery.Variants.PERFORMANCE)\n",\
    "    temp_viz_file = 'dfg.png'\n",\
    "    pm4py.save_vis_dfg(dfg, start_activities, end_activities, temp_viz_file, log=None)\n",\
    "    model['plot_pairplot'] = pngfile_to_base64(temp_viz_file)\n",\
    "        \n",\
    "    # apply inductive miner for petri net retrival\n",\
    "    net, initial_marking, final_marking = inductive_miner.apply(event_logs)\n",\
    "    temp_viz_file = 'petrinet.png'\n",\
    "    pm4py.save_vis_petri_net(net, initial_marking, final_marking, temp_viz_file)\n",\
    "    model['plot_matrix'] = pngfile_to_base64(temp_viz_file)\n",\
    "    \n",\
    "    # Other options:\n",\
    "    # discover process tree\n",\
    "    #process_tree = pm4py.discover_tree_inductive(event_logs)\n",\
    "    #pm4py.save_vis_process_tree(process_tree, 'processtree.png')\n",\
    "\n",\
    "    # add frequency information\n",\
    "    #dfg_frequency = dfg_discovery.apply(event_logs, variant=dfg_discovery.Variants.FREQUENCY)\n",\
    "\n",\
    "    return pd.DataFrame(list(net.arcs))"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 57,\
   "metadata": {},\
   "outputs": [\
    {\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "                                 0\n",\
      "0               (t)skip_10->(p)p_3\n",\
      "1              (t)skip_13->(p)p_12\n",\
      "2   (t)splunk-system-user->(p)p_13\n",\
      "3             (p)p_8->(t)tauJoin_4\n",\
      "4                (t)tau_1->(p)sink\n",\
      "5          (p)p_4->(t)init_loop_11\n",\
      "6           (t)tauSplit_3->(p)p_10\n",\
      "7   (p)p_12->(t)splunk-system-user\n",\
      "8               (t)skip_14->(p)p_3\n",\
      "9               (t)skip_9->(p)p_11\n",\
      "10           (t)tauSplit_3->(p)p_7\n",\
      "11               (t)skip_2->(p)p_4\n",\
      "12            (p)source->(t)skip_2\n",\
      "13               (p)p_8->(t)skip_7\n",\
      "14              (p)p_10->(t)skip_9\n",\
      "15               (t)skip_7->(p)p_7\n",\
      "16                (p)p_7->(t)admin\n",\
      "17        (p)source->(t)tauSplit_3\n",\
      "18                (t)admin->(p)p_8\n",\
      "19             (p)p_13->(t)skip_14\n",\
      "20        (t)init_loop_11->(p)p_12\n",\
      "21             (p)p_13->(t)skip_13\n",\
      "22           (p)p_11->(t)tauJoin_4\n",\
      "23            (t)tauJoin_4->(p)p_4\n",\
      "24                   (t)-->(p)p_11\n",\
      "25              (p)p_4->(t)skip_10\n",\
      "26                (p)p_3->(t)tau_1\n",\
      "27                   (p)p_10->(t)-\n"\
     ]\
    }\
   ],\
   "source": [\
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing or development purposes\n",\
    "result = apply(model,df,param)\n",\
    "print(result)"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 51,\
   "metadata": {},\
   "outputs": [\
    {\
     "data": {\
      "text/plain": [\
       "\"b'iVBORw0K\""\
      ]\
     },\
     "execution_count": 51,\
     "metadata": {},\
     "output_type": "execute_result"\
    }\
   ],\
   "source": [\
    "model['plot_matrix'][:10]"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 5 - save the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 46,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_save"\
   },\
   "outputs": [],\
   "source": [\
    "# save model to name in expected convention \"algo_<model_name>\"\n",\
    "def save(model,name):\n",\
    "    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n",\
    "        json.dump(model, file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "saved_model = save(model,'algo_barebone_model')\n",\
    "saved_model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 6 - load the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_load"\
   },\
   "outputs": [],\
   "source": [\
    "# load model from name in expected convention \"algo_<model_name>\"\n",\
    "def load(name):\n",\
    "    model = {}\n",\
    "    with open(MODEL_DIRECTORY + name + \".json\", 'r') as file:\n",\
    "        model = json.load(file)\n",\
    "    return model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": [\
    "loaded_model = load('algo_barebone_model')\n",\
    "loaded_model"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## Stage 7 - provide a summary of the model"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 8,\
   "metadata": {\
    "deletable": false,\
    "name": "dltk_summary"\
   },\
   "outputs": [],\
   "source": [\
    "# return a model summary\n",\
    "def summary(model=None):\n",\
    "    returns = {\"version\": {\"pm4py\": pm4py.__version__} }\n",\
    "    return returns"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": 38,\
   "metadata": {},\
   "outputs": [\
    {\
     "data": {\
      "text/plain": [\
       "{'version': {'pm4py': '2.0.1.3'}}"\
      ]\
     },\
     "execution_count": 38,\
     "metadata": {},\
     "output_type": "execute_result"\
    }\
   ],\
   "source": [\
    "summary(model)"\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "Finally you can reuse the model by calling your SPL with | compute ... method=\"apply\""\
   ]\
  },\
  {\
   "cell_type": "markdown",\
   "metadata": {},\
   "source": [\
    "## End of Stages\n",\
    "All subsequent cells are not tagged and can be used for further freeform code"\
   ]\
  },\
  {\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": {},\
   "outputs": [],\
   "source": []\
  }\
 ],\
 "metadata": {\
  "kernelspec": {\
   "display_name": "Python 3",\
   "language": "python",\
   "name": "python3"\
  },\
  "language_info": {\
   "codemirror_mode": {\
    "name": "ipython",\
    "version": 3\
   },\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython3",\
   "version": "3.7.6"\
  }\
 },\
 "nbformat": 4,\
 "nbformat_minor": 4\
}\

